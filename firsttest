# Databricks notebook source
# MAGIC %md-sandbox
# MAGIC <div style="text-align: center; line-height: 0; padding-top: 9px;">
# MAGIC   <img src="https://cdn2.hubspot.net/hubfs/438089/docs/training/dblearning-banner.png" alt="Databricks Learning" width="555" height="64">
# MAGIC </div>

# COMMAND ----------

# MAGIC %md #Introduction to Caching and Partitioning
# MAGIC 
# MAGIC ** Data Source **
# MAGIC * One hour of Pagecounts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.
# MAGIC * Size on Disk: ~23 MB
# MAGIC * Type: Compressed Parquet File
# MAGIC * More Info: <a href="https://dumps.wikimedia.org/other/pagecounts-raw" target="_blank">Page view statistics for Wikimedia projects</a>
# MAGIC 
# MAGIC **Technical Accomplishments:**
# MAGIC * Introduce caching
# MAGIC * Partitioning
# MAGIC * Writing to files

# COMMAND ----------

# MAGIC %md We are going to start by reading in a Parquet file in the Azure blob store.
# MAGIC 
# MAGIC See this [documentation](https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-storage.html#azure-blob-storage) for connecting to Azure data sources.

# COMMAND ----------

# spark.conf.set(
#   "fs.azure.account.key.{YOUR STORAGE ACCOUNT NAME}.blob.core.windows.net",
#   "{YOUR STORAGE ACCOUNT ACCESS KEY}")

spark.conf.set(
  "fs.azure.account.key.spearfishtrainingstorage.blob.core.windows.net",
  "cJoNObBZt8C8xz2GwQmaHx25DmyRXAyd8TEp7/HDlT6jgt4+LeOjwYEhQ5SsCCrO0HRy6xlL8WZEM6xEwE9+9Q==")

# dbutils.fs.ls("wasbs://{YOUR CONTAINER NAME}@{YOUR STORAGE ACCOUNT NAME}.blob.core.windows.net/{YOUR DIRECTORY NAME}")

dbutils.fs.ls("wasbs://training-container@spearfishtrainingstorage.blob.core.windows.net/wikipedia_pagecounts_en.parquet")

# COMMAND ----------

pagecountsEnAllDF = (spark                  # Our SparkSession & Entry Point
                    .read                   # Our DataFrameReader
                    .parquet("wasbs://training-container@spearfishtrainingstorage.blob.core.windows.net/wikipedia_pagecounts_en.parquet"))

# COMMAND ----------

# MAGIC %md We can see how many partitions our data is stored in.

# COMMAND ----------

pagecountsEnAllDF.rdd.getNumPartitions()

# COMMAND ----------

# MAGIC %md Why did we get this number of partitions? Let's look at the `Executors` tab in the Spark UI.

# COMMAND ----------

# MAGIC %md 
# MAGIC Every time we re-run these operations, it goes all the way back to the data source.
# MAGIC 
# MAGIC This requires pulling all the data across the network for every execution.
# MAGIC 
# MAGIC In many cases, this network IO is the most expensive part of a job.

# COMMAND ----------

# MAGIC %md ##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) persist()
# MAGIC 
# MAGIC `cache()` is just an alias for `persist()`
# MAGIC 
# MAGIC Let's take a look at the API docs for
# MAGIC * `Dataset.persist(..)` if using Scala
# MAGIC * `DataFrame.persist(..)` if using Python
# MAGIC 
# MAGIC Other Storage Levels:
# MAGIC * DISK_ONLY
# MAGIC * DISK_ONLY_2
# MAGIC * MEMORY_AND_DISK
# MAGIC * MEMORY_AND_DISK_2
# MAGIC * MEMORY_AND_DISK_SER
# MAGIC * MEMORY_AND_DISK_SER_2
# MAGIC * MEMORY_ONLY
# MAGIC * MEMORY_ONLY_2
# MAGIC * MEMORY_ONLY_SER
# MAGIC * MEMORY_ONLY_SER_2
# MAGIC * OFF_HEAP
# MAGIC 
# MAGIC ** *Note:* ** *The default storage level for...*
# MAGIC * *RDDs are **MEMORY_ONLY**.*
# MAGIC * *DataFrames are **MEMORY_AND_DISK**.* 
# MAGIC * *Streaming is **MEMORY_AND_DISK_2**.*

# COMMAND ----------

from pyspark import StorageLevel

# Default persistance level
pagecountsEnAllDF.persist(StorageLevel.MEMORY_AND_DISK) # Alternatively, pagecountsEnAllDF.cache()

# COMMAND ----------

# MAGIC %md Let's trigger an action on our DataFrame, such as `show`.

# COMMAND ----------

pagecountsEnAllDF.show()

# COMMAND ----------

# MAGIC %md Why was only 1 partition cached?
# MAGIC 
# MAGIC Let's try another action, such as `count`.

# COMMAND ----------

pagecountsEnAllDF.count()

# COMMAND ----------

# MAGIC %md Let's re-run the same code down below and take a look at how long it takes to execute compared to the previous cell.

# COMMAND ----------

pagecountsEnAllDF.count()

# COMMAND ----------

# MAGIC %md ##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Tungsten
# MAGIC 
# MAGIC Tungsten is the in-memory storage format for Spark SQL / DataFrames. Advantages:
# MAGIC 
# MAGIC * Compactness: Column values are encoded using custom encoders, not as JVM objects (as with RDDs). The benefit of using Spark 2.x's custom encoders is that you get almost the same compactness as Java serialization, but significantly faster encoding/decoding speeds. Also, for custom data types, it is possible to write custom encoders from scratch.
# MAGIC 
# MAGIC * Efficiency: Spark can operate _directly out of Tungsten_, without deserializing Tungsten data into JVM objects first.
# MAGIC 
# MAGIC To see the advantages of Tungsten, let's convert our DataFrame into an RDD and cache it. Then, compare the size in memory of the RDD and the DataFrame.

# COMMAND ----------

rdd = pagecountsEnAllDF.rdd
rdd.setName("myRdd").cache()
rdd.count() # Trigger an action

# COMMAND ----------

# MAGIC %md ##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Unpersist
# MAGIC 
# MAGIC Notice that RDDs take up more space, and they are serialized in Python (so you have to spend time to deserialize them).
# MAGIC 
# MAGIC Let's remove the RDD from cache using `unpersist()`. 
# MAGIC 
# MAGIC NOTE: `cache()` is an alias for `persist()`, but there is no alias for `unpersist()`.

# COMMAND ----------

rdd.unpersist()

# COMMAND ----------

# MAGIC %md ##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Views
# MAGIC 
# MAGIC Now that I have (hopefully) convinced you to use DataFrames over RDDs, here is another neat feature of DataFrames: you can register them as views, and then query them using any of the supported Spark Language APIs. 
# MAGIC 
# MAGIC ** *Note #1:* ** *The method createOrReplaceTempView(..) is bound to the SparkSession meaning it will be discarded once the session ends.*
# MAGIC 
# MAGIC ** *Note #2:* ** On the other hand, the method createOrReplaceGlobalTempView(..) is bound to the spark application.*
# MAGIC 
# MAGIC *Or to put that another way, I can use createOrReplaceTempView(..) in this notebook only, however, I can call createOrReplaceGlobalTempView(..) in this notebook and then access it from another.*

# COMMAND ----------

pagecountsEnAllDF.createOrReplaceTempView("pagecountsView")

# COMMAND ----------

# MAGIC %md Another nice feature of working with views/tables is that you can cache them with a cleaner name in the Spark UI.
# MAGIC 
# MAGIC But first, we would need to unpersist the underlying DataFrame from cache.

# COMMAND ----------

pagecountsEnAllDF.unpersist()
spark.catalog.cacheTable("pagecountsView")

# COMMAND ----------

# MAGIC %md Let's do a count to see how many records we have (there are a few different ways to do this).

# COMMAND ----------

# MAGIC %sql SELECT count(*) FROM pagecountsView

# COMMAND ----------

spark.read.table("pagecountsView").count()

# COMMAND ----------

# MAGIC %md ##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Write to File
# MAGIC Finally, we can write out our DataFrame as a Parquet file. But before we do that, let's check how many partitions we have.

# COMMAND ----------

pagecountsEnAllDF.rdd.getNumPartitions()

# COMMAND ----------

# MAGIC %md This means that when we write out to a file, it will write a file for each of our 8 partitions. We have a few ways to deal with this.

# COMMAND ----------

# MAGIC %md ##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) repartition(n) or coalesce(n)
# MAGIC 
# MAGIC We have two operations that can help address this problem: `repartition(n)` and `coalesce(n)`.
# MAGIC 
# MAGIC If you look at the API docs, `coalesce(n)` is described like this:
# MAGIC > Returns a new Dataset that has exactly numPartitions partitions, when fewer partitions are requested.<br/>
# MAGIC > If a larger number of partitions is requested, it will stay at the current number of partitions.
# MAGIC 
# MAGIC If you look at the API docs, `repartition(n)` is described like this:
# MAGIC > Returns a new Dataset that has exactly numPartitions partitions.
# MAGIC 
# MAGIC The key differences between the two are
# MAGIC * `coalesce(n)` is a **narrow** transformation and can only be used to reduce the number of partitions.
# MAGIC * `repartition(n)` is a **wide** transformation and can be used to reduce or increase the number of partitions.
# MAGIC 
# MAGIC So, if I'm increasing the number of partitions I have only one choice: `repartition(n)`
# MAGIC 
# MAGIC If I'm reducing the number of partitions I can use either one, so how do I decide?
# MAGIC * First off, `coalesce(n)` is a **narrow** transformation and performs better because it avoids a shuffle.
# MAGIC * However, `coalesce(n)` cannot guarantee even **distribution of records** across all partitions.
# MAGIC * For example, with `coalesce(n)` you might end up with **a few partitions containing 80%** of all the data.
# MAGIC * On the other hand, `repartition(n)` will give us a relatively **uniform distribution**.
# MAGIC * And `repartition(n)` is a **wide** transformation meaning we have the added cost of a **shuffle operation**.

# COMMAND ----------

# MAGIC %md **Narrow Transformations** are able to perform the computation using only the data contained within the partition of data assigned to it.
# MAGIC 
# MAGIC Examples include:
# MAGIC * `filter(..)`
# MAGIC * `drop(..)`
# MAGIC * `coalesce()`
# MAGIC 
# MAGIC <img src="https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/transformations-narrow.png" alt="Narrow Transformations" style="height:300px"/>
# MAGIC 
# MAGIC <br/>
# MAGIC 
# MAGIC **Wide Transformations** are those type of transformations that require comparing data in multiple partitions. 
# MAGIC 
# MAGIC Examples include:
# MAGIC * `distinct()` 
# MAGIC * `groupBy(..).sum()` 
# MAGIC * `repartition(n)` 
# MAGIC 
# MAGIC <img src="https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/transformations-wide.png" alt="Wide Transformations" style="height:300px"/>

# COMMAND ----------

# MAGIC %md ### More or Less Partitions?
# MAGIC 
# MAGIC As a **general guideline** it is advised that each partition (when cached) is roughly around 200MB.
# MAGIC * Size on disk is not a good gauge. For example...
# MAGIC * CSV files are large on disk but small in RAM - consider the string "12345" which is 10 bytes compared to the integer 12345 which is only 4 bytes.
# MAGIC * Parquet files are highly compressed but uncompressed in RAM.
# MAGIC 
# MAGIC The **200 comes from** the real-world-experience of Databricks's engineers and is **based largely on efficiency** and not so much resource limitations. 
# MAGIC 
# MAGIC The goal will **ALWAYS** be to use as few partitions as possible while maintaining at least 1 x number-of-slots.

# COMMAND ----------

# MAGIC %md Let's go ahead and coalesce our DataFrame into 4 partitions so we only write out 4 files.

# COMMAND ----------

coalescedDF = pagecountsEnAllDF.coalesce(4)

# COMMAND ----------

# MAGIC %md Now let's write out `coalescedDF` to `dbfs:/tmp/pagecounts.parquet`
# MAGIC 
# MAGIC We are going to use the `Overwrite` mode in case you re-run this notebook.

# COMMAND ----------

coalescedDF.write.mode("overwrite").parquet("dbfs:/tmp/pagecounts.parquet")

# COMMAND ----------

# MAGIC %md Let's verify that it wrote properly.

# COMMAND ----------

display( dbutils.fs.ls("dbfs:/tmp/pagecounts.parquet") )

# COMMAND ----------

# MAGIC %md Notice the problem with coalesce - it wrote out in 4 files, but they are not of roughly equal sizes. 
# MAGIC 
# MAGIC Let's instead do a repartition before writing out our files, and increase it to 15 partitions. 

# COMMAND ----------

pagecountsEnAllDF.repartition(15).write.mode("overwrite").parquet("dbfs:/tmp/pagecounts.parquet")

# COMMAND ----------

# MAGIC %md ##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Read from File
# MAGIC 
# MAGIC Look at the API docs to see how to read our file back in from Parquet, then check the resulting number of partitions. Is it surprising? Why?
# MAGIC 
# MAGIC [DataFrameReader Python](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)

# COMMAND ----------

# TODO
df13 = spark.read.parquet("dbfs:/tmp/pagecounts.parquet")

# COMMAND ----------

# MAGIC %md ##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Bonus
# MAGIC As a bonus, we can also write it out to the blob store directly.
# MAGIC 
# MAGIC Make sure to write out to a unique path name (i.e. your first and last name).

# COMMAND ----------

# TODO